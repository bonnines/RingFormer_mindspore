{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341d2bfb-663d-4d6b-8199-a7c2e86d0d5a",
   "metadata": {},
   "source": [
    "# 基于MindSpore的RingFormer实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba5afa-830f-408c-843f-aa80723bf9cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HeteroTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c05ab4-82ba-45b1-80c0-c2dc23c05f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.3, while the latest version is 1.3.6.\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/graphgym/config.py:19: UserWarning: Could not define global config object. Please install 'yacs' via 'pip install yacs' in order to use GraphGym\n",
      "  warnings.warn(\"Could not define global config object. Please install \"\n"
     ]
    }
   ],
   "source": [
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore import Tensor\n",
    "from mindspore.common.initializer import initializer, Normal, XavierUniform\n",
    "from mindspore.nn import Cell, SequentialCell, ReLU\n",
    "from mindspore import context\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import numpy as np\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from mindspore_gl import BatchedGraph, BatchedGraphField\n",
    "from torch_geometric.nn import GINConv, global_add_pool, GCNConv, global_mean_pool, dense_diff_pool, DenseGINConv, GPSConv\n",
    "from torch_geometric.nn.models import MLP, AttentiveFP\n",
    "from torch_geometric.utils import remove_self_loops\n",
    "\n",
    "class SparseEdgeConv(Cell):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True, beta=False, \n",
    "                 dropout=0., bias=True, root_weight=True, combine='add', clip_attn=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.beta = beta and root_weight\n",
    "        self.root_weight = root_weight\n",
    "        self.concat = concat\n",
    "        self.dropout = dropout\n",
    "        self.combine = combine\n",
    "        self.clip_attn = clip_attn\n",
    "        \n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        self.lin_key = nn.Dense(in_channels[0], heads * out_channels)\n",
    "        self.lin_query = nn.Dense(in_channels[1], heads * out_channels)\n",
    "        self.lin_value = nn.Dense(in_channels[0], heads * out_channels)\n",
    "\n",
    "        if self.combine.startswith('cat'):\n",
    "            if self.combine[-1] == '1':\n",
    "                self.lin_combine = nn.SequentialCell([nn.Dense(in_channels[0]*2, in_channels[0])])\n",
    "            elif self.combine[-1] == '2':\n",
    "                self.lin_combine = nn.SequentialCell([\n",
    "                    nn.Dense(in_channels[0]*2, in_channels[0]), \n",
    "                    nn.Dropout(dropout)\n",
    "                ])\n",
    "            else:\n",
    "                self.lin_combine = nn.SequentialCell([\n",
    "                    nn.Dense(in_channels[0]*2, in_channels[0]), \n",
    "                    nn.ReLU()\n",
    "                ])\n",
    "        elif self.combine.startswith('add_lin'):\n",
    "            self.lin_combine = nn.SequentialCell([\n",
    "                nn.Dense(in_channels[0], in_channels[0]), \n",
    "                nn.ReLU()\n",
    "            ])\n",
    "        elif self.combine.startswith('lin_add'):\n",
    "            self.lin_combine = nn.Dense(in_channels[0], in_channels[0])\n",
    "        elif self.combine.startswith('dual_lin_add'):\n",
    "            self.lin_combine0 = nn.Dense(in_channels[0], in_channels[0])\n",
    "            self.lin_combine1 = nn.Dense(in_channels[0], in_channels[0])\n",
    "            \n",
    "        if concat:\n",
    "            self.lin_skip = nn.Dense(in_channels[1], heads * out_channels, has_bias=bias)\n",
    "            if self.beta:\n",
    "                self.lin_beta = nn.Dense(3 * heads * out_channels, 1, has_bias=False)\n",
    "            else:\n",
    "                self.lin_beta = None\n",
    "        else:\n",
    "            self.lin_skip = nn.Dense(in_channels[1], out_channels, has_bias=bias)\n",
    "            if self.beta:\n",
    "                self.lin_beta = nn.Dense(3 * out_channels, 1, has_bias=False)\n",
    "            else:\n",
    "                self.lin_beta = None\n",
    "                \n",
    "        self.softmax = ops.Softmax(axis=-1)\n",
    "        self.dropout_op = nn.Dropout(dropout)\n",
    "        self._alpha = None\n",
    "\n",
    "    def construct(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "        if isinstance(x, Tensor):\n",
    "            x = (x, x)\n",
    "\n",
    "        # Propagate\n",
    "        out = self.propagate(x, edge_index, edge_attr)\n",
    "        \n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(axis=1)\n",
    "\n",
    "        if self.root_weight:\n",
    "            x_r = self.lin_skip(x[1])\n",
    "            out = out + x_r\n",
    "        else:\n",
    "            out = out + x[1]\n",
    "\n",
    "        if return_attention_weights:\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, query_i, key_j, value_j, edge_attr, index, ptr, size_i):\n",
    "        assert edge_attr is not None\n",
    "        \n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        if self.combine == 'add':\n",
    "            key_j = value_j = key_j + edge_attr\n",
    "        elif self.combine.startswith('cat'):\n",
    "            key_j = value_j = self.lin_combine(ops.concat([key_j, edge_attr], axis=-1))\n",
    "        elif self.combine == 'add_lin':\n",
    "            key_j = value_j = self.lin_combine(key_j + edge_attr)\n",
    "        elif self.combine == 'lin_add':\n",
    "            edge_attr = self.lin_combine(edge_attr)\n",
    "            key_j = value_j = ops.relu(key_j + edge_attr)\n",
    "        elif self.combine.startswith('dual_lin_add'):\n",
    "            if self.combine[-1] == '1':\n",
    "                edge_attr = self.lin_combine0(edge_attr)\n",
    "                key_j, value_j = self.lin_combine0(key_j), self.lin_combine0(value_j)\n",
    "                key_j = ops.relu(key_j + edge_attr)\n",
    "                value_j = ops.relu(value_j + edge_attr)\n",
    "            elif self.combine[-1] == '2':\n",
    "                edge_attr = self.lin_combine0(edge_attr)\n",
    "                key_j, value_j = self.lin_combine0(key_j), self.lin_combine0(value_j)\n",
    "                key_j = key_j + edge_attr\n",
    "                value_j = value_j + edge_attr\n",
    "            elif self.combine[-1] == '3':\n",
    "                edge_attr = self.lin_combine0(edge_attr)\n",
    "                key_j, value_j = self.lin_combine0(key_j), self.lin_combine0(value_j)\n",
    "                key_j = self.dropout_op(key_j + edge_attr)\n",
    "                value_j = self.dropout_op(value_j + edge_attr)\n",
    "            elif self.combine[-1] == '4':\n",
    "                edge_attr = self.lin_combine0(edge_attr)\n",
    "                key_j = value_j = self.lin_combine1(key_j) + edge_attr\n",
    "            elif self.combine[-1] == '5':\n",
    "                edge_attr = self.lin_combine0(edge_attr)\n",
    "                key_j = value_j = ops.relu(self.lin_combine1(key_j) + edge_attr)\n",
    "        \n",
    "        query_i = self.lin_query(query_i).view(-1, H, C)\n",
    "        key_j = self.lin_key(key_j).view(-1, H, C)\n",
    "        value_j = self.lin_value(value_j).view(-1, H, C)\n",
    "\n",
    "        alpha = (query_i * key_j).sum(axis=-1) / math.sqrt(self.out_channels)\n",
    "        if self.clip_attn:\n",
    "            alpha = ops.clip_by_value(alpha, -5, 5)\n",
    "        alpha = self.softmax(alpha)\n",
    "        self._alpha = alpha\n",
    "        alpha = self.dropout_op(alpha)\n",
    "\n",
    "        out = value_j * alpha.view(-1, self.heads, 1)\n",
    "        return out\n",
    "\n",
    "class SparseEdgeFullLayer(Cell):\n",
    "    def __init__(self, in_dim, out_dim, num_heads, dropout=0.0, dim_edge=None, \n",
    "                 layer_norm=True, activation='relu', root_weight=True, residual=True, \n",
    "                 use_bias=False, combine='add', clip_attn=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_dim\n",
    "        self.out_channels = out_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.layer_norm = layer_norm\n",
    "\n",
    "        self.attention = SparseEdgeConv(in_dim, out_dim//num_heads, heads=num_heads, \n",
    "                                      root_weight=root_weight, dropout=dropout, concat=True, \n",
    "                                      use_bias=use_bias, combine=combine, clip_attn=clip_attn)\n",
    "\n",
    "        self.O_h = nn.Dense(out_dim, out_dim)\n",
    "\n",
    "        if self.layer_norm:\n",
    "            self.layer_norm1_h = nn.LayerNorm((out_dim,))\n",
    "\n",
    "        # FFN for h\n",
    "        self.FFN_h_layer1 = nn.Dense(out_dim, out_dim * 2)\n",
    "        self.activation_fn = self._get_activation(activation)\n",
    "        self.FFN_h_layer2 = nn.Dense(out_dim * 2 if activation != 'glu' else out_dim, out_dim)\n",
    "\n",
    "        if self.layer_norm:\n",
    "            self.layer_norm2_h = nn.LayerNorm((out_dim,))\n",
    "\n",
    "        self.dropout_op = nn.Dropout(dropout)\n",
    "\n",
    "    def _get_activation(self, activation):\n",
    "        if activation == 'relu':\n",
    "            return ops.ReLU()\n",
    "        elif activation == 'gelu':\n",
    "            return ops.GeLU()\n",
    "        elif activation == 'silu':\n",
    "            return ops.SiLU()\n",
    "        elif activation == 'glu':\n",
    "            return ops.GLU()\n",
    "        else:\n",
    "            raise ValueError(f'activation function {activation} is not valid!')\n",
    "\n",
    "    def construct(self, x, edge_index, edge_attr, **kwargs):\n",
    "        h = x\n",
    "        h_in1 = h  # for first residual connection\n",
    "\n",
    "        # multi-head attention out\n",
    "        h = self.attention(x, edge_index, edge_attr)\n",
    "        h = self.dropout_op(h)\n",
    "        h = self.O_h(h)\n",
    "\n",
    "        if self.residual:\n",
    "            h = h_in1 + h  # residual connection\n",
    "\n",
    "        if self.layer_norm:\n",
    "            h = self.layer_norm1_h(h)\n",
    "\n",
    "        h_in2 = h  # for second residual connection\n",
    "\n",
    "        # FFN for h\n",
    "        h = self.FFN_h_layer1(h)\n",
    "        h = self.activation_fn(h)\n",
    "        h = self.dropout_op(h)\n",
    "        h = self.FFN_h_layer2(h)\n",
    "\n",
    "        if self.residual:\n",
    "            h = h_in2 + h  # residual connection\n",
    "\n",
    "        if self.layer_norm:\n",
    "            h = self.layer_norm2_h(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "class Het_Transfomer(Cell):\n",
    "    def __init__(self, metadata, dim, num_gc_layers, gnn='GINE', inter_gnn='GINE', ring_gnn='GPS', \n",
    "                 norm=None, transformer_norm=None, aggr='sum', jk='cat', dropout=0.0, attn_dropout=0.0, \n",
    "                 pool='add', first_residual=False, residual=False, heads=4, use_bias=False, padding=True, \n",
    "                 init_embs=False, mask_non_edge=False, add_mol=False, combine_mol='add', root_weight=True, \n",
    "                 combine_edge='add', clip_attn=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_gc_layers = num_gc_layers\n",
    "        self.convs = nn.CellList()\n",
    "        self.jk = jk\n",
    "        self.dropout = dropout\n",
    "        self.residual = residual\n",
    "        self.first_residual = first_residual\n",
    "        self.aggr = aggr\n",
    "        self.use_edge_attr = True\n",
    "        self.ring_gnn = ring_gnn\n",
    "        self.add_mol = add_mol\n",
    "        self.combine_mol = combine_mol\n",
    "        \n",
    "        assert norm is None\n",
    "        \n",
    "        if 'mol' in metadata[0]:\n",
    "            self.use_mol = True\n",
    "            print('Adding Mol node to heterogenous graph!')\n",
    "        else:\n",
    "            self.use_mol = False\n",
    "        if 'pair' in metadata[0]:\n",
    "            self.use_pair = True\n",
    "            print('Adding Pair node to heterogenous graph!')\n",
    "        else:\n",
    "            self.use_pair = False\n",
    "            \n",
    "        self.pool = global_add_pool\n",
    "            \n",
    "        if 'cat' in aggr:\n",
    "            self.lin_atom = nn.CellList()\n",
    "            self.lin_ring = nn.CellList()\n",
    "            if self.use_mol:\n",
    "                self.lin_mol = nn.CellList()\n",
    "            if self.use_pair:\n",
    "                self.lin_pair = nn.CellList()\n",
    "                \n",
    "        # Initialize GNN layers based on configuration\n",
    "        # (Implementation of specific GNN layers would be needed here)\n",
    "        \n",
    "        num_atom_messages = 0\n",
    "        num_ring_messages = 0\n",
    "        num_pair_messages = 0\n",
    "        for rel in metadata[1]:\n",
    "            if rel[-1] == 'atom':\n",
    "                num_atom_messages += 1\n",
    "            elif rel[-1] == 'ring':\n",
    "                num_ring_messages += 1\n",
    "            elif rel[-1] == 'pair':\n",
    "                num_pair_messages += 1\n",
    "                \n",
    "        for _ in range(num_gc_layers):\n",
    "            # Initialize conv_dict with appropriate GNN layers\n",
    "            conv_dict = {}\n",
    "            conv = HeteroConv(conv_dict, aggr='cat' if 'cat' in aggr else aggr)\n",
    "            self.convs.append(conv)\n",
    "            \n",
    "            if aggr == 'cat':\n",
    "                self.lin_atom.append(nn.SequentialCell([\n",
    "                    nn.Dense(num_atom_messages*dim, dim), \n",
    "                    nn.ReLU()\n",
    "                ]))\n",
    "                self.lin_ring.append(nn.SequentialCell([\n",
    "                    nn.Dense(num_ring_messages*dim, dim), \n",
    "                    nn.ReLU()\n",
    "                ]))\n",
    "                if self.use_mol:\n",
    "                    self.lin_mol.append(nn.SequentialCell([\n",
    "                        nn.Dense(dim, dim), \n",
    "                        nn.ReLU()\n",
    "                    ]))\n",
    "                if self.use_pair:\n",
    "                    self.lin_pair.append(nn.SequentialCell([\n",
    "                        nn.Dense(dim*num_pair_messages, dim), \n",
    "                        nn.ReLU()\n",
    "                    ]))\n",
    "            elif aggr == 'cat_self':\n",
    "                self.lin_atom.append(nn.SequentialCell([\n",
    "                    nn.Dense((num_atom_messages+1)*dim, dim), \n",
    "                    nn.ReLU()\n",
    "                ]))\n",
    "                self.lin_ring.append(nn.SequentialCell([\n",
    "                    nn.Dense((num_ring_messages+1)*dim, dim), \n",
    "                    nn.ReLU()\n",
    "                ]))\n",
    "                if self.use_mol:\n",
    "                    self.lin_mol.append(nn.SequentialCell([\n",
    "                        nn.Dense(2*dim, dim), \n",
    "                        nn.ReLU()\n",
    "                    ]))\n",
    "                if self.use_pair:\n",
    "                    self.lin_pair.append(nn.SequentialCell([\n",
    "                        nn.Dense(2*dim, dim), \n",
    "                        nn.ReLU()\n",
    "                    ]))\n",
    "    \n",
    "    def construct(self, x_dict, edge_index_dict, batch_dict, edge_attr_dict=None, edge_type_dict=None, data=None):\n",
    "        x_atom = [x_dict['atom']] if self.first_residual else []\n",
    "        x_ring = [x_dict['ring']] if self.first_residual else []\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.use_edge_attr:\n",
    "                x_dict = conv(x_dict, edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "            else:\n",
    "                x_dict = conv(x_dict, edge_index_dict)\n",
    "                \n",
    "            x_dict = {key: self.dropout_op(ops.relu(x)) for key, x in x_dict.items()}\n",
    "            \n",
    "            if self.aggr == 'cat':\n",
    "                x_dict['atom'] = self.dropout_op(self.lin_atom[i](x_dict['atom']))\n",
    "                x_dict['ring'] = self.dropout_op(self.lin_ring[i](x_dict['ring']))\n",
    "            elif self.aggr == 'cat_self':\n",
    "                x_dict['atom'] = self.dropout_op(self.lin_atom[i](ops.concat((x_atom[-1], x_dict['atom']), -1)))\n",
    "                x_dict['ring'] = self.dropout_op(self.lin_ring[i](ops.concat((x_ring[-1], x_dict['ring']), -1)))\n",
    "                \n",
    "            x_atom.append(x_dict['atom'])\n",
    "            x_ring.append(x_dict['ring'])\n",
    "            \n",
    "        if self.jk == 'cat':\n",
    "            x_atom = ops.concat(x_atom, 1)\n",
    "            x_ring = ops.concat(x_ring, 1)\n",
    "        elif self.jk == 'last':\n",
    "            x_atom = x_atom[-1]\n",
    "            x_ring = x_ring[-1]\n",
    "            \n",
    "        x_atom = self.pool(x_atom, batch_dict['atom'])\n",
    "        \n",
    "        if self.add_mol:\n",
    "            x_ring_out = self.pool(x_ring[data['ring'].ring_mask], batch_dict['ring'][data['ring'].ring_mask])\n",
    "            x_mol = self.pool(x_ring[~data['ring'].ring_mask], batch_dict['ring'][~data['ring'].ring_mask])\n",
    "            \n",
    "            if self.combine_mol == 'add':\n",
    "                x_ring_out = x_ring_out + x_mol\n",
    "            elif self.combine_mol == 'cat':\n",
    "                x_ring_out = ops.concat((x_ring_out, x_mol), -1)\n",
    "            elif self.combine_mol == 'drop':\n",
    "                pass\n",
    "        else:\n",
    "            x_ring_out = self.pool(x_ring, batch_dict['ring'])\n",
    "            x_mol = None\n",
    "            \n",
    "        return x_atom, x_ring_out, None, x_mol\n",
    "    \n",
    "\n",
    "class BondEncoder(nn.Cell):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(BondEncoder, self).__init__()\n",
    "        \n",
    "        self.bond_embedding_list = nn.CellList()\n",
    "        full_bond_feature_dims = [22, 6, 2]\n",
    "        for i, dim in enumerate(full_bond_feature_dims):\n",
    "            emb = nn.Embedding(dim, emb_dim)\n",
    "            emb.embedding_table.set_data(initializer(XavierUniform(), emb.embedding_table.shape))\n",
    "            self.bond_embedding_list.append(emb)\n",
    "\n",
    "    def construct(self, edge_attr):\n",
    "        bond_embedding = 0\n",
    "        for i in range(edge_attr.shape[1]):\n",
    "            bond_embedding += self.bond_embedding_list[i](edge_attr[:,i])\n",
    "\n",
    "        return bond_embedding\n",
    "\n",
    "class RingEncoder(nn.Cell):\n",
    "    def __init__(self, emb_dim, pe=False):\n",
    "        super(RingEncoder, self).__init__()\n",
    "        \n",
    "        self.ring_embedding_list = nn.CellList()\n",
    "        full_ring_feature_dims = [60]\n",
    "        for i, dim in enumerate(full_ring_feature_dims):\n",
    "            emb = nn.Embedding(dim+1, emb_dim)\n",
    "            emb.embedding_table.set_data(initializer(XavierUniform(), emb.embedding_table.shape))\n",
    "            self.ring_embedding_list.append(emb)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x_embedding = 0\n",
    "        for i in range(x.shape[1]):\n",
    "            x_embedding += self.ring_embedding_list[i](x[:,i])\n",
    "\n",
    "        return x_embedding\n",
    "    \n",
    "class RingBondDegreeEncoder(nn.Cell):\n",
    "    def __init__(self, emb_dim, num_edge_types=17):\n",
    "        super(RingBondDegreeEncoder, self).__init__()\n",
    "        \n",
    "        self.ring_embedding_list = nn.CellList()\n",
    "        full_ring_feature_dims = [7]*num_edge_types\n",
    "        for i, dim in enumerate(full_ring_feature_dims):\n",
    "            emb = nn.Embedding(dim+1, emb_dim, padding_idx=0)\n",
    "            emb.embedding_table.set_data(initializer(XavierUniform(), emb.embedding_table.shape))\n",
    "            emb.embedding_table[0] = 0.0\n",
    "            self.ring_embedding_list.append(emb)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x_embedding = 0\n",
    "        for i in range(x.shape[1]):\n",
    "            x_embedding += self.ring_embedding_list[i](x[:,i])\n",
    "\n",
    "        return x_embedding\n",
    "\n",
    "class Attention(nn.Cell):\n",
    "    def __init__(self, in_size, hidden_size=16):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.project = SequentialCell(\n",
    "            nn.Dense(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dense(hidden_size, 1, has_bias=False)\n",
    "        )\n",
    "\n",
    "    def construct(self, z):\n",
    "        w = self.project(z)\n",
    "        beta = ops.softmax(w, axis=1)\n",
    "        return (beta * z).sum(1).squeeze(), beta\n",
    "\n",
    "class HeteroTransformer(nn.Cell):\n",
    "    def __init__(self, metadata,\n",
    "                 nclass,\n",
    "                 nhid=128, \n",
    "                 nlayer=5,\n",
    "                 dropout=0, \n",
    "                 attn_dropout=0.0,\n",
    "                 norm=None, \n",
    "                 transformer_norm=None,\n",
    "                 heads=4,\n",
    "                 pool='add',\n",
    "                 conv='GINE',\n",
    "                 inter_conv='GINE',\n",
    "                 ring_conv='GINE',\n",
    "                 jk='cat',\n",
    "                 final_jk='cat',\n",
    "                 intra_jk='cat',\n",
    "                 aggr='cat',\n",
    "                 criterion='MSE',\n",
    "                 normalize=False,\n",
    "                 residual=False,\n",
    "                 target_task=None,\n",
    "                 ring_init='atom_deepset',\n",
    "                 mol_init='atom_deepset',\n",
    "                 pair_init='random',\n",
    "                 pe_dim=0,\n",
    "                 pe_emb_dim=128,\n",
    "                 num_lin_layer=1,\n",
    "                 model='Het',\n",
    "                 contrastive=False,\n",
    "                 num_deepset_layer=1,\n",
    "                 init_embs=False,\n",
    "                 padding=True,\n",
    "                 mask_non_edge=False,\n",
    "                 cat_pe=False,\n",
    "                 use_bias=False,\n",
    "                 add_mol=False,\n",
    "                 combine_mol='add',\n",
    "                 float_pe=False,\n",
    "                 combine_edge='add',\n",
    "                 root_weight=True,\n",
    "                 num_ring_edge_types=1,\n",
    "                 clip_attn=False,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.normalize = normalize\n",
    "        self.target_task = target_task\n",
    "        self.pe_dim = pe_dim\n",
    "        self.final_jk = final_jk\n",
    "        self.ring_init = ring_init\n",
    "        self.mol_init = mol_init\n",
    "        self.contrastive = contrastive\n",
    "        self.cat_pe = cat_pe\n",
    "        self.add_mol = add_mol\n",
    "        self.float_pe = float_pe\n",
    "        self.num_ring_edge_types = num_ring_edge_types\n",
    "            \n",
    "        first_residual = True\n",
    "        Encoder = Het_Transfomer  # You'll need to implement this in MindSpore\n",
    "\n",
    "        self.encoder = Encoder(metadata, dim=nhid, gnn=conv, inter_gnn=inter_conv, ring_gnn=ring_conv, \n",
    "                             num_gc_layers=nlayer, heads=heads, norm=norm, transformer_norm=transformer_norm, \n",
    "                             dropout=dropout, attn_dropout=attn_dropout, pool=pool,\n",
    "                             aggr=aggr, jk=jk, intra_jk=intra_jk, first_residual=first_residual, \n",
    "                             init_embs=init_embs, padding=padding, mask_non_edge=mask_non_edge, \n",
    "                             residual=residual, use_bias=use_bias, add_mol=add_mol, combine_mol=combine_mol, \n",
    "                             root_weight=root_weight, combine_edge=combine_edge, clip_attn=clip_attn)\n",
    "        \n",
    "        # AtomEncoder needs to be implemented in MindSpore\n",
    "        self.atom_encoder = AtomEncoder(nhid)  \n",
    "        self.ring_encoder = RingEncoder(nhid-pe_emb_dim) if cat_pe else RingEncoder(nhid)\n",
    "        \n",
    "        # Edge attr encoder\n",
    "        ring_bond_encoder = nn.Embedding(42, nhid)\n",
    "        ring_bond_encoder.embedding_table.set_data(initializer(XavierUniform(), ring_bond_encoder.embedding_table.shape))\n",
    "        ar_bond_encoder = nn.Embedding(2, nhid)\n",
    "        ar_bond_encoder.embedding_table.set_data(initializer(XavierUniform(), ar_bond_encoder.embedding_table.shape))\n",
    "        ra_bond_encoder = nn.Embedding(2, nhid)\n",
    "        ra_bond_encoder.embedding_table.set_data(initializer(XavierUniform(), ra_bond_encoder.embedding_table.shape))\n",
    "          \n",
    "        self.bond_encoder = nn.CellDict({'a2a': BondEncoder(nhid), 'a2r': ar_bond_encoder, \n",
    "                                       'r2r': ring_bond_encoder, 'r2a': ra_bond_encoder})\n",
    "        \n",
    "        if self.add_mol:\n",
    "            self.mol_encoder = nn.Embedding(2, nhid-pe_emb_dim) if cat_pe else nn.Embedding(2, nhid)\n",
    "            self.mol_encoder.embedding_table.set_data(initializer(XavierUniform(), self.mol_encoder.embedding_table.shape))\n",
    "                    \n",
    "        if pe_dim > 0:\n",
    "            if float_pe:\n",
    "                self.pe_encoder = SequentialCell(nn.Dense(pe_dim, pe_emb_dim)) if cat_pe else SequentialCell(nn.Dense(pe_dim, nhid))\n",
    "            else:\n",
    "                if num_ring_edge_types == 1:\n",
    "                    self.pe_encoder = nn.Embedding(pe_dim+1, pe_emb_dim, padding_idx=0) if cat_pe else nn.Embedding(pe_dim+1, nhid, padding_idx=0)\n",
    "                    self.pe_encoder.embedding_table.set_data(initializer(XavierUniform(), self.pe_encoder.embedding_table.shape))\n",
    "                    self.pe_encoder.embedding_table[0] = 0.0\n",
    "                else:\n",
    "                    self.pe_encoder = RingBondDegreeEncoder(pe_emb_dim, num_ring_edge_types) if cat_pe else RingBondDegreeEncoder(nhid, num_ring_edge_types)\n",
    "        \n",
    "        if ring_init.startswith('atom_deepset') or ring_init == 'deepset_random':\n",
    "            self.ring_deepset = SequentialCell(nn.Dense(nhid, nhid-pe_emb_dim), ReLU()) if cat_pe else SequentialCell(nn.Dense(nhid, nhid), ReLU())\n",
    "            \n",
    "        penultimate_dim = (nlayer+1)*nhid if jk == 'cat' else nhid\n",
    "        if final_jk == 'cat':\n",
    "            final_dim = penultimate_dim * 2\n",
    "            if combine_mol == 'cat':\n",
    "                final_dim = final_dim + penultimate_dim\n",
    "        else:\n",
    "            final_dim = penultimate_dim\n",
    "            \n",
    "        if num_lin_layer == 1:\n",
    "            self.lin = nn.Dense(final_dim, nclass)\n",
    "        else:\n",
    "            self.lin = SequentialCell(\n",
    "                nn.Dense(final_dim, penultimate_dim),\n",
    "                ReLU(),\n",
    "                nn.Dropout(p=dropout),\n",
    "                nn.Dense(penultimate_dim, nclass)\n",
    "            )\n",
    "        \n",
    "        if criterion == 'MSE':\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif criterion == 'MAE':\n",
    "            self.criterion = nn.L1Loss()\n",
    "        else:\n",
    "            raise NameError(f\"{criterion} is not implemented!\")\n",
    "\n",
    "    def construct(self, data):\n",
    "        # Initialize node embeddings\n",
    "        # Atom\n",
    "        x_atom = self.atom_encoder(data.x_dict['atom'].astype(ms.int32))\n",
    "        \n",
    "        # Ring\n",
    "        if self.ring_init == 'random':\n",
    "            x_ring = self.ring_encoder(data.x_dict['ring'].astype(ms.int32))\n",
    "        elif self.ring_init == 'zero':\n",
    "            x_ring = ops.zeros((data['ring'].ptr[-1].item(), x_atom.shape[1]), x_atom.dtype)\n",
    "        elif self.ring_init.startswith('atom_deepset') or self.ring_init == 'deepset_random':\n",
    "            ringatoms_batch = [ops.full((n,), i, ms.int32) for i, n in enumerate(data.num_ringatoms)]\n",
    "            ringatoms_batch = ops.concat(ringatoms_batch, axis=0)\n",
    "            ringatoms_ptr = data['atom'].ptr[ringatoms_batch]\n",
    "            ringatoms = data.ring_atoms + ringatoms_ptr\n",
    "            ring_atoms_map = data.ring_atoms_map + data['ring'].ptr[ringatoms_batch]\n",
    "            \n",
    "            x_ring = global_add_pool(x_atom[ringatoms], ring_atoms_map)\n",
    "            x_ring = ops.dropout(self.ring_deepset(x_ring), p=self.dropout, training=self.training)\n",
    "            \n",
    "            if self.add_mol:\n",
    "                x_ring = ops.concat((x_ring, self.mol_encoder(Tensor([0], ms.int32))), 0)\n",
    "        elif self.ring_init == 'add' or self.ring_init == 'mean':\n",
    "            ringatoms_batch = [ops.full((n,), i, ms.int32) for i, n in enumerate(data.num_ringatoms)]\n",
    "            ringatoms_batch = ops.concat(ringatoms_batch, axis=0)\n",
    "            ringatoms_ptr = data['atom'].ptr[ringatoms_batch]\n",
    "            ringatoms = data.ring_atoms + ringatoms_ptr\n",
    "            ring_atoms_map = data.ring_atoms_map + data['ring'].ptr[ringatoms_batch]\n",
    "            \n",
    "            if self.ring_init == 'add':\n",
    "                x_ring = global_add_pool(x_atom[ringatoms], ring_atoms_map)\n",
    "            elif self.ring_init == 'mean':\n",
    "                x_ring = global_mean_pool(x_atom[ringatoms], ring_atoms_map)\n",
    "        \n",
    "        if self.pe_dim > 0:\n",
    "            if self.cat_pe:\n",
    "                if not self.float_pe:\n",
    "                    if self.num_ring_edge_types == 1:\n",
    "                        x_ring = ops.concat((x_ring, self.pe_encoder(data['ring'].ring_pe.reshape(-1).astype(ms.int32))), -1)\n",
    "                    else:\n",
    "                        x_ring = ops.concat((x_ring, self.pe_encoder(data['ring'].ring_pe.astype(ms.int32))), -1)\n",
    "                else:\n",
    "                    x_ring = ops.concat((x_ring, self.pe_encoder(data['ring'].ring_pe)), -1)\n",
    "            else:\n",
    "                if not self.float_pe:\n",
    "                    if self.num_ring_edge_types == 1:\n",
    "                        x_ring = x_ring + self.pe_encoder(data['ring'].ring_pe.reshape(-1).astype(ms.int32))\n",
    "                    else:\n",
    "                        x_ring = x_ring + self.pe_encoder(data['ring'].ring_pe.astype(ms.int32))\n",
    "                else:\n",
    "                    x_ring = x_ring + self.pe_encoder(data['ring'].ring_pe)\n",
    "        \n",
    "        x_dict = {'atom': x_atom, 'ring': x_ring}\n",
    "        edge_attr_dict = {edge_type: self.bond_encoder[edge_type[1]](edge_attr) \n",
    "                         for edge_type, edge_attr in data.edge_attr_dict.items()}\n",
    "        \n",
    "        atom_embs, ring_embs, pair_embs, mol_embs = self.encoder(\n",
    "            x_dict, data.edge_index_dict, data.batch_dict, edge_attr_dict, \n",
    "            edge_type_dict=data.edge_attr_dict, data=data)\n",
    "        \n",
    "        return atom_embs, ring_embs, pair_embs, mol_embs\n",
    "    \n",
    "    def get_embs(self, data):\n",
    "        atom_embs, ring_embs, pair_embs, mol_embs = self(data)\n",
    "        if self.final_jk == 'cat':\n",
    "            graph_embs = ops.concat([atom_embs, ring_embs], axis=1)\n",
    "        elif self.final_jk == 'add':\n",
    "            graph_embs = atom_embs + ring_embs\n",
    "        elif self.final_jk == 'attention':\n",
    "            graph_embs = [atom_embs, ring_embs]\n",
    "            graph_embs = ops.stack(graph_embs, axis=1)\n",
    "            graph_embs, attn_values = self.final_attn(graph_embs)\n",
    "        elif self.final_jk == 'attention_param':\n",
    "            graph_embs = [atom_embs, ring_embs]\n",
    "            graph_embs = ops.stack(graph_embs, axis=1)\n",
    "            graph_embs = (graph_embs * ops.softmax(self.final_attn, axis=1)).sum(1)\n",
    "        elif self.final_jk == 'atom':\n",
    "            graph_embs = atom_embs\n",
    "        elif self.final_jk == 'ring':\n",
    "            graph_embs = ring_embs\n",
    "        elif self.final_jk == 'mol':\n",
    "            graph_embs = mol_embs\n",
    "        else:\n",
    "            raise NameError(f\"{self.final_jk} is not implemented!\")\n",
    "        return graph_embs\n",
    "    \n",
    "    def predict_score(self, data):\n",
    "        atom_embs, ring_embs, pair_embs, mol_embs = self(data)\n",
    "        if self.final_jk == 'cat':\n",
    "            graph_embs = ops.concat([atom_embs, ring_embs], axis=1)\n",
    "        elif self.final_jk == 'add':\n",
    "            graph_embs = atom_embs + ring_embs\n",
    "        elif self.final_jk == 'attention':\n",
    "            graph_embs = [atom_embs, ring_embs]\n",
    "            graph_embs = ops.stack(graph_embs, axis=1)\n",
    "            graph_embs, attn_values = self.final_attn(graph_embs)\n",
    "        elif self.final_jk == 'attention_param':\n",
    "            graph_embs = [atom_embs, ring_embs]\n",
    "            graph_embs = ops.stack(graph_embs, axis=1)\n",
    "            graph_embs = (graph_embs * ops.softmax(self.final_attn, axis=1)).sum(1)\n",
    "        elif self.final_jk == 'atom':\n",
    "            graph_embs = atom_embs\n",
    "        elif self.final_jk == 'ring':\n",
    "            graph_embs = ring_embs\n",
    "        elif self.final_jk == 'mol':\n",
    "            graph_embs = mol_embs\n",
    "        else:\n",
    "            raise NameError(f\"{self.final_jk} is not implemented!\")\n",
    "\n",
    "        scores = self.lin(graph_embs)\n",
    "        return scores\n",
    "    \n",
    "    def calc_contra_loss(self, data):\n",
    "        atom_embs, ring_embs, pair_embs, mol_embs = self(data)\n",
    "        g1, g2 = [self.project(g) for g in [atom_embs, ring_embs]]\n",
    "        loss = self.ssl_criterion(g1=g1, g2=g2)\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss(self, data):\n",
    "        scores = self.predict_score(data)\n",
    "        mask = (data.y != 0).astype(ms.float32)\n",
    "        scores = scores * mask\n",
    "        loss = self.criterion(scores, data.y)\n",
    "        return loss\n",
    "    \n",
    "def global_add_pool(x: Tensor, batch: Tensor = None, size: int = None) -> Tensor:\n",
    "    \"\"\"MindSpore implementation of global additive pooling.\n",
    "    \n",
    "    Args:\n",
    "        x (Tensor): Node feature matrix with shape [N, F] or [N]\n",
    "        batch (Tensor, optional): Batch vector assigning each node to a graph. Shape [N]\n",
    "        size (int, optional): Number of graphs in batch. If None, inferred from batch.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Graph-level outputs with shape [B, F] or [B]\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        dim = -1\n",
    "    else:\n",
    "        dim = -2\n",
    "    \n",
    "    if batch is None:\n",
    "        return x.sum(axis=dim, keepdims=x.ndim <= 2)\n",
    "    \n",
    "    if size is None:\n",
    "        size = int(batch.max().asnumpy().item()) + 1\n",
    "    \n",
    "    # MindSpore's unsorted_segment_sum requires int32 batch indices\n",
    "    batch = batch.astype(ms.int32)\n",
    "    \n",
    "    if x.ndim == 1:\n",
    "        return ops.unsorted_segment_sum(x, batch, size)\n",
    "    else:\n",
    "        return ops.unsorted_segment_sum(x, batch, size)\n",
    "\n",
    "def global_mean_pool(x: Tensor, batch: Tensor = None, size: int = None) -> Tensor:\n",
    "    \"\"\"MindSpore implementation of global mean pooling.\n",
    "    \n",
    "    Args:\n",
    "        x (Tensor): Node feature matrix with shape [N, F] or [N]\n",
    "        batch (Tensor, optional): Batch vector assigning each node to a graph. Shape [N]\n",
    "        size (int, optional): Number of graphs in batch. If None, inferred from batch.\n",
    "        \n",
    "    Returns:\n",
    "        Tensor: Graph-level outputs with shape [B, F] or [B]\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        dim = -1\n",
    "    else:\n",
    "        dim = -2\n",
    "    \n",
    "    if batch is None:\n",
    "        return x.mean(axis=dim, keepdims=x.ndim <= 2)\n",
    "    \n",
    "    if size is None:\n",
    "        size = int(batch.max().asnumpy().item()) + 1\n",
    "    \n",
    "    # MindSpore's unsorted_segment_sum requires int32 batch indices\n",
    "    batch = batch.astype(ms.int32)\n",
    "    \n",
    "    if x.ndim == 1:\n",
    "        sum_pool = ops.unsorted_segment_sum(x, batch, size)\n",
    "        counts = ops.unsorted_segment_sum(ops.ones_like(x), batch, size)\n",
    "    else:\n",
    "        sum_pool = ops.unsorted_segment_sum(x, batch, size)\n",
    "        counts = ops.unsorted_segment_sum(ops.ones_like(x[:, 0]), batch, size)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    counts = ops.maximum(counts, ops.ones_like(counts))\n",
    "    \n",
    "    if x.ndim == 1:\n",
    "        return sum_pool / counts\n",
    "    else:\n",
    "        return sum_pool / counts.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa41a3-ad7c-4af7-b089-143fc5cd3228",
   "metadata": {},
   "source": [
    "## 模型训练及评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca76ff4e-f71c-4e0b-be1e-38b93acd1bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deepchem.models:Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "WARNING:deepchem.models:Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-add_mol'], dest='add_mol', nargs=None, const=None, default=True, type=<class 'bool'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import deepchem as dc\n",
    "import warnings\n",
    "import deepchem as dc\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "from mindspore.train.callback import Callback\n",
    "from mindspore.common.initializer import initializer, Normal\n",
    "\n",
    "from data_loader_het import get_dataset_het\n",
    "from copy import deepcopy\n",
    "import mindspore.dataset as ds\n",
    "from mindspore import context\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def setup_seed(seed):\n",
    "    ms.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    context.set_context(mode=context.GRAPH_MODE)\n",
    "\n",
    "class SchedulerCallback(Callback):\n",
    "    def __init__(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def on_train_epoch_end(self, run_context):\n",
    "        cb_params = run_context.original_args()\n",
    "        self.scheduler.step()\n",
    "\n",
    "def train(args, filename=None):\n",
    "    # pytorch\n",
    "    # device = torch.device('cuda:%d' % args.gpu)\n",
    "    \n",
    "    # mindspore\n",
    "    context.set_context(device_target=\"GPU\" if args.gpu >= 0 else \"CPU\", device_id=args.gpu)\n",
    "    \n",
    "    maes, mapes, mses = [], [], []\n",
    "    best_vals = []\n",
    "\n",
    "    float_pe = False\n",
    "    pe_dim = 7\n",
    "    num_ring_edge_types = 1\n",
    "    add_mol = False\n",
    "\n",
    "    transform = T.Compose([AddHetRingDegreePE(pe_dim), AddVirtualMol()])\n",
    "    add_mol = True\n",
    "\n",
    "    dataloader, dataloader_test, dataloader_val, transformer, meta = get_dataset_het(args, transform)\n",
    "    num_classes = meta['num_classes']\n",
    "    n_train = len(dataloader.get_dataset_size())\n",
    "    n_val = len(dataloader_val.get_dataset_size())\n",
    "    n_test = len(dataloader_test.get_dataset_size())\n",
    "        \n",
    "    for trial in range(args.num_trial):\n",
    "        setup_seed(trial)\n",
    "        # Model initialization\n",
    "        model = HeteroTransformer(dataloader.dataset[0].metadata(), num_classes, args.hidden_dim, args.num_layer, \n",
    "                                heads=args.heads, conv=args.model, ring_conv=args.ring_conv, pool=args.pool, \n",
    "                                norm='BatchNorm' if args.bn else args.norm, \n",
    "                                transformer_norm=args.transformer_norm, l2norm=args.l2norm, \n",
    "                                dropout=args.dropout, attn_dropout=args.attn_dropout, \n",
    "                                criterion=args.criterion, jk=args.jk, final_jk=args.final_jk, \n",
    "                                aggr=args.aggr, normalize=args.normalize, \n",
    "                                first_residual=args.first_residual, residual=args.residual, \n",
    "                                ring_init=args.ring_init, pe_dim=pe_dim, cat_pe=args.cat_pe, \n",
    "                                use_bias=args.use_bias, add_mol=add_mol, combine_mol=args.combine_mol, \n",
    "                                float_pe=float_pe, combine_edge=args.combine_edge, \n",
    "                                root_weight=args.root_weight, num_ring_edge_types=num_ring_edge_types, \n",
    "                                clip_attn=args.clip_attn, model='Transformer')\n",
    "        \n",
    "        optimizer = nn.Adam(model.trainable_params(), learning_rate=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "        if args.scheduler.startswith('step'):\n",
    "            step_size, gamma = args.scheduler.split('-')[1:]\n",
    "            scheduler = nn.StepLR(optimizer, step_size=int(step_size), gamma=float(gamma))\n",
    "        elif args.scheduler == 'cosine':\n",
    "            scheduler = nn.CosineDecayLR(args.lr, args.num_epoch)\n",
    "        elif args.scheduler.startswith('onecycle'):\n",
    "            pct_start = float(args.scheduler.split('-')[1]) if '-' in args.scheduler else 0.1\n",
    "            scheduler = nn.OneCycleLR(args.lr, args.num_epoch, steps_per_epoch=len(dataloader), pct_start=pct_start)\n",
    "        else:\n",
    "            scheduler = None\n",
    "            \n",
    "        scheduler_cb = SchedulerCallback(scheduler) if scheduler else None\n",
    "        \n",
    "        # Training & Validation\n",
    "        best_val = float(\"Inf\")\n",
    "        best_epoch = 0\n",
    "        best_model_params = None\n",
    "        \n",
    "        for epoch in range(1, args.num_epoch + 1):\n",
    "            model.set_train()\n",
    "            loss_all = 0\n",
    "            \n",
    "            for data in dataloader.create_dict_iterator():\n",
    "                loss = model.calc_loss(data)\n",
    "                optimizer.clear_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_all += loss.asnumpy() * data['num_graphs'].asnumpy()\n",
    "                \n",
    "            if scheduler_cb:\n",
    "                scheduler_cb.on_train_epoch_end(None)\n",
    "                \n",
    "            print('[TRAIN] Epoch:{:03d} | Loss:{:.4f}'.format(epoch, loss_all / n_train))\n",
    "            \n",
    "            # Validation\n",
    "            model.set_train(False)\n",
    "            loss_all_val = 0.0\n",
    "            \n",
    "            for data in dataloader_val.create_dict_iterator():\n",
    "                loss = model.calc_loss(data)\n",
    "                loss_all_val += loss.asnumpy() * data['num_graphs'].asnumpy()\n",
    "                \n",
    "            if loss_all_val < best_val:\n",
    "                best_val = loss_all_val\n",
    "                best_model_params = deepcopy(model.parameters_dict())\n",
    "                best_epoch = epoch\n",
    "\n",
    "            if epoch % args.eval_freq == 0:\n",
    "                model.set_train(False)\n",
    "                y_true = []\n",
    "                y_preds = []\n",
    "                \n",
    "                for data in dataloader_test.create_dict_iterator():\n",
    "                    y_true.extend(data['y'].asnumpy().reshape(-1, num_classes).tolist())\n",
    "                    y_preds.extend(model.predict_score(data).asnumpy().reshape(-1, num_classes).tolist())\n",
    "                    \n",
    "                y_true = np.array(y_true)\n",
    "                y_preds = np.array(y_preds)\n",
    "                test_mask = y_true != 0\n",
    "                y_true = y_true[test_mask].reshape(-1, 1).tolist()\n",
    "                y_preds = y_preds[test_mask].reshape(-1, 1).tolist()\n",
    "                \n",
    "                if args.normalize:\n",
    "                    y_true = transformer.inverse_transform(y_true)\n",
    "                    y_preds = transformer.inverse_transform(y_preds)\n",
    "                    \n",
    "                mae = mean_absolute_error(y_true, y_preds)\n",
    "                mape = mean_absolute_percentage_error(y_true, y_preds)\n",
    "                mse = mean_squared_error(y_true, y_preds)\n",
    "                \n",
    "        # Test on best validation\n",
    "        ms.load_param_into_net(model, best_model_params)\n",
    "        model.set_train(False)\n",
    "        y_true = []\n",
    "        y_preds = []\n",
    "        \n",
    "        for data in dataloader_test.create_dict_iterator():\n",
    "            y_true.extend(data['y'].asnumpy().reshape(-1, num_classes).tolist())\n",
    "            y_preds.extend(model.predict_score(data).asnumpy().reshape(-1, num_classes).tolist())\n",
    "            \n",
    "        assert len(y_true) == n_test and len(y_preds) == n_test\n",
    "        y_true = np.array(y_true)\n",
    "        y_preds = np.array(y_preds)\n",
    "        test_mask = y_true != 0\n",
    "        y_true = y_true[test_mask].reshape(-1, 1).tolist()\n",
    "        y_preds = y_preds[test_mask].reshape(-1, 1).tolist()\n",
    "        \n",
    "        if args.normalize:\n",
    "            y_true = transformer.inverse_transform(y_true)\n",
    "            y_preds = transformer.inverse_transform(y_preds)\n",
    "            \n",
    "        mae = mean_absolute_error(y_true, y_preds)\n",
    "        mape = mean_absolute_percentage_error(y_true, y_preds)\n",
    "        mse = mean_squared_error(y_true, y_preds)\n",
    "\n",
    "        maes.append(mae)\n",
    "        mapes.append(mape)\n",
    "        mses.append(mse)\n",
    "        best_vals.append(best_val)\n",
    "\n",
    "    avg_val = np.mean(maes)\n",
    "    std_val = np.std(maes)\n",
    "    print('MAE: {:.4f}+-{:.4f}'.format(avg_val, std_val))\n",
    "    \n",
    "def get_dataset_het_ms(args, transform=None):\n",
    "    \"\"\"MindSpore版本的数据加载函数\"\"\"\n",
    "    meta = {}\n",
    "    transformer = None\n",
    "    \n",
    "    # 特征化器选择（与原始代码相同）\n",
    "    if args.featurizer == 'MACCS':\n",
    "        featurizer = MACCSKeysFingerprint()\n",
    "    elif args.featurizer == 'ECFP6':\n",
    "        featurizer = CircularFingerprint(size=1024, radius=6)\n",
    "    elif args.featurizer == 'Mordred':\n",
    "        featurizer = MordredDescriptors(ignore_3D=True)\n",
    "    elif args.featurizer is None:\n",
    "        featurizer = None\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # 数据集加载（需要适配MindSpore）\n",
    "    if args.dataset == 'HOPV':\n",
    "        dataset_pyg = HOPVHetDataset(transform=transform, version=args.dataset_version)\n",
    "        index_dir = './data/HOPV/'\n",
    "    elif args.dataset == 'PFD':\n",
    "        dataset_pyg = PolymerFAHetDataset(transform=transform, version=args.dataset_version)\n",
    "        index_dir = './data/PFD/'\n",
    "    elif args.dataset == 'PD':\n",
    "        dataset_pyg = pNFAHetDataset(transform=transform, version=args.dataset_version)\n",
    "        index_dir = './data/Polymer_NFA_p/'\n",
    "    elif args.dataset == 'NFA':\n",
    "        dataset_pyg = nNFAHetDataset(transform=transform, version=args.dataset_version)\n",
    "        index_dir = './data/Polymer_NFA_n/'\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # 特征处理\n",
    "    X = featurizer.featurize(dataset_pyg.data.smiles) if featurizer else np.arange(len(dataset_pyg)).reshape(-1,1)\n",
    "    meta['fingerprint_dim'] = X.shape[1]\n",
    "    \n",
    "    # 目标数据处理\n",
    "    if args.target_mode == 'single':\n",
    "        nonzero_mask = dataset_pyg.data.y[:, args.target_task] > -100 if (args.dataset == 'HOPV' and args.target_task == 0) else dataset_pyg.data.y[:, args.target_task] != 0\n",
    "        smiles = np.array(dataset_pyg.data.smiles)[nonzero_mask].tolist()\n",
    "        dataset = dc_data.DiskDataset.from_numpy(\n",
    "            X[nonzero_mask], \n",
    "            dataset_pyg.data.y.numpy()[nonzero_mask, args.target_task], \n",
    "            None, \n",
    "            smiles\n",
    "        )\n",
    "        meta['num_classes'] = 1\n",
    "    elif args.target_mode == 'multi':\n",
    "        dataset = dc_data.DiskDataset.from_numpy(\n",
    "            X, \n",
    "            dataset_pyg.data.y, \n",
    "            None, \n",
    "            dataset_pyg.data.smiles\n",
    "        )\n",
    "        meta['num_classes'] = dataset.y.shape[1]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    # 数据集划分\n",
    "    splitter = RandomSplitter() if args.splitter == 'random' else ScaffoldSplitter()\n",
    "    train_index, valid_index, test_index = splitter.split(\n",
    "        dataset, \n",
    "        frac_train=args.frac_train, \n",
    "        frac_valid=(1-args.frac_train)/2, \n",
    "        frac_test=(1-args.frac_train)/2\n",
    "    )\n",
    "    \n",
    "    # 数据标准化\n",
    "    if args.normalize:\n",
    "        if args.scaler == 'standard':\n",
    "            transformer = StandardScaler()\n",
    "        elif args.scaler == 'minmax':\n",
    "            transformer = MinMaxScaler()\n",
    "        transformer.fit(train_dataset.y.reshape(-1, meta['num_classes']))\n",
    "        y_train = transformer.transform(train_dataset.y.reshape(-1, meta['num_classes']))\n",
    "        y_valid = transformer.transform(valid_dataset.y.reshape(-1, meta['num_classes']))\n",
    "        y_test = transformer.transform(test_dataset.y.reshape(-1, meta['num_classes']))\n",
    "    else:\n",
    "        y_train = train_dataset.y.reshape(-1, meta['num_classes'])\n",
    "        y_valid = valid_dataset.y.reshape(-1, meta['num_classes'])\n",
    "        y_test = test_dataset.y.reshape(-1, meta['num_classes'])\n",
    "    \n",
    "    # 创建MindSpore数据集\n",
    "    def data_generator(indices, y_data, mode='train'):\n",
    "        for idx, y in zip(indices, y_data):\n",
    "            data = dataset_pyg[idx]\n",
    "            data.y = ms.Tensor(np.array([y]), dtype=ms.float32).reshape(1, -1)\n",
    "            yield self._convert_to_mindspore_format(data)\n",
    "    \n",
    "    # 使用GeneratorDataset创建数据集\n",
    "    train_ds = ds.GeneratorDataset(\n",
    "        source=lambda: data_generator(train_index, y_train),\n",
    "        column_names=[\"data\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_ds = ds.GeneratorDataset(\n",
    "        source=lambda: data_generator(valid_index, y_valid),\n",
    "        column_names=[\"data\"],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_ds = ds.GeneratorDataset(\n",
    "        source=lambda: data_generator(test_index, y_test),\n",
    "        column_names=[\"data\"],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 批量处理\n",
    "    train_ds = train_ds.batch(args.batch_size)\n",
    "    val_ds = val_ds.batch(1024)\n",
    "    test_ds = test_ds.batch(1024)\n",
    "    \n",
    "    return train_ds, test_ds, val_ds, transformer, meta\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-dataset', type=str, default='HOPV', choices=['HOPV', 'PFD', 'NFA', 'PD', 'CEPDB'])\n",
    "    parser.add_argument('-dataset_version', type=str, default='V1')\n",
    "    parser.add_argument('-featurizer', type=str, default=None, choices=[None, 'MACCS', 'ECFP6', 'Mordred'])\n",
    "    parser.add_argument('-normalize', type=bool, default=False)\n",
    "    parser.add_argument('-scaler', type=str, default='standard', choices=['minmax', 'standard'])\n",
    "    parser.add_argument('-frac_train', type=float, default=0.6)\n",
    "    parser.add_argument('-target_mode', type=str, default='single')\n",
    "    parser.add_argument('-target_task', type=int, default=0, help='0: PCE, 1: HOMO, 2: LUMO, 3: band gap, 4: Voc, 5: Jsc, 6: FF')\n",
    "    parser.add_argument('-splitter', type=str, default='scaffold')\n",
    "    parser.add_argument('-model', type=str, default='GINE')\n",
    "    parser.add_argument('-ring_conv', type=str, default='SparseEdge')\n",
    "    parser.add_argument('-num_trial', type=int, default=1)\n",
    "    parser.add_argument('-gpu', type=int, default=1)\n",
    "\n",
    "    parser.add_argument('-num_epoch', type=int, default=10)\n",
    "    parser.add_argument('-eval_freq', type=int, default=10)\n",
    "    parser.add_argument('-batch_size', type=int, default=128)\n",
    "    parser.add_argument('-bn', type=bool, default=False)\n",
    "    parser.add_argument('-norm', type=str, default=None, choices=[None, 'BatchNorm', 'LayerNorm'])\n",
    "    parser.add_argument('-transformer_norm', type=str, default='LayerNorm', choices=[None, 'BatchNorm', 'LayerNorm'])\n",
    "\n",
    "\n",
    "    parser.add_argument('-lr', type=float, default=0.001)\n",
    "    parser.add_argument('-weight_decay', type=float, default=5e-4)\n",
    "    parser.add_argument('-dropout', type=float, default=0.0)\n",
    "    parser.add_argument('-attn_dropout', type=float, default=0.0)\n",
    "    parser.add_argument('-criterion', type=str, default='MAE')\n",
    "    parser.add_argument('-scheduler', type=str, default='onecycle-0.05')\n",
    "\n",
    "    parser.add_argument('-num_layer', type=int, default=5)\n",
    "    parser.add_argument('-hidden_dim', type=int, default=128)\n",
    "    parser.add_argument('-heads', type=int, default=4)\n",
    "    parser.add_argument('-l2norm', type=bool, default=False)\n",
    "    parser.add_argument('-pool', type=str, default='add')\n",
    "    parser.add_argument('-jk', type=str, default='cat')\n",
    "    parser.add_argument('-final_jk', type=str, default='cat')\n",
    "    parser.add_argument('-aggr', type=str, default='cat')\n",
    "    parser.add_argument('-ring_init', type=str, default='random')\n",
    "    parser.add_argument('-first_residual', type=bool, default=True)\n",
    "    parser.add_argument('-residual', type=bool, default=True)\n",
    "    parser.add_argument('-use_bias', type=bool, default=False)\n",
    "\n",
    "    parser.add_argument('-transform', type=str, default=None, choices=[None, 'VirtualNode'])\n",
    "    parser.add_argument('-best_val', type=bool, default=True)\n",
    "\n",
    "    parser.add_argument('-PE', type=str, default='RingDegree', choices=['RingDegree', 'RingBondDegree', 'RandomWalk']) # \n",
    "    parser.add_argument('-pe_dim', type=int, default=7)\n",
    "    parser.add_argument('-cat_pe', type=bool, default=True)\n",
    "\n",
    "    parser.add_argument('-combine_mol', type=str, default='add')\n",
    "    parser.add_argument('-root_weight', type=bool, default=True)\n",
    "    parser.add_argument('-combine_edge', type=str, default='cat', choices=['add', 'add_lin', 'cat','add_lin'])\n",
    "    parser.add_argument('-clip_attn', type=bool, default=True)\n",
    "    parser.add_argument('-add_cross', type=bool, default=True)\n",
    "    parser.add_argument('-add_mol', type=bool, default=True)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args()\n",
    "    train(args, None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8028fc58-c79c-4e6e-9eb2-a159ef29b2a1",
   "metadata": {},
   "source": [
    "## PFD数据集复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db3715ac-7cea-4c44-a8ef-85b3869dc8b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch:010 | Loss:1.9042\n",
      "[TRAIN] Epoch:020 | Loss:1.6362\n",
      "[TRAIN] Epoch:030 | Loss:1.4894\n",
      "[TRAIN] Epoch:040 | Loss:1.2977\n",
      "[EVAL] trial:0 | MAE:1.8805\n",
      "[TRAIN] Epoch:010 | Loss:2.0529\n",
      "[TRAIN] Epoch:020 | Loss:1.7544\n",
      "[TRAIN] Epoch:030 | Loss:1.7681\n",
      "[TRAIN] Epoch:040 | Loss:1.3465\n",
      "[EVAL] trial:1 | MAE:1.7271\n",
      "[TRAIN] Epoch:010 | Loss:2.1798\n",
      "[TRAIN] Epoch:020 | Loss:1.6379\n",
      "[TRAIN] Epoch:030 | Loss:1.3286\n",
      "[TRAIN] Epoch:040 | Loss:1.7871\n",
      "[EVAL] trial:2 | MAE:1.7265\n",
      "[FINAL EVAL] MAE: 1.7781+-0.0725\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [\n",
    "    \"train.py\",          \n",
    "    \"-dataset\", \"PFD\",\n",
    "    \"-num_layer\", \"8\",\n",
    "    \"-hidden_dim\", \"512\",\n",
    "    \"-heads\", \"4\",\n",
    "    \"-num_epoch\", \"40\",\n",
    "    \"-batch_size\", \"32\",\n",
    "    \"-lr\", \"0.01\",\n",
    "    \"-num_trial\", \"3\"\n",
    "]\n",
    "\n",
    "args = parser.parse_args()                \n",
    "train(args, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9891ac9-82b7-405a-9bed-ab87c4f02cbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## HOPV数据集复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40ecb36-d5a5-435a-8e53-b73fcfbbd474",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py38/lib/python3.8/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch:010 | Loss:2.3964\n",
      "[TRAIN] Epoch:020 | Loss:1.7668\n",
      "[TRAIN] Epoch:030 | Loss:1.5432\n",
      "[TRAIN] Epoch:040 | Loss:1.3757\n",
      "[TRAIN] Epoch:050 | Loss:1.1448\n",
      "[TRAIN] Epoch:060 | Loss:1.1641\n",
      "[TRAIN] Epoch:070 | Loss:1.1050\n",
      "[TRAIN] Epoch:080 | Loss:1.1042\n",
      "[TRAIN] Epoch:090 | Loss:1.2098\n",
      "[TRAIN] Epoch:100 | Loss:1.0693\n",
      "[TRAIN] Epoch:110 | Loss:1.0667\n",
      "[TRAIN] Epoch:120 | Loss:1.0903\n",
      "[TRAIN] Epoch:130 | Loss:1.0695\n",
      "[TRAIN] Epoch:140 | Loss:1.0076\n",
      "[TRAIN] Epoch:150 | Loss:0.9961\n",
      "[EVAL] trial:0 | MAE:1.6005\n",
      "[TRAIN] Epoch:010 | Loss:2.3770\n",
      "[TRAIN] Epoch:020 | Loss:1.6895\n",
      "[TRAIN] Epoch:030 | Loss:1.5439\n",
      "[TRAIN] Epoch:040 | Loss:1.4042\n",
      "[TRAIN] Epoch:050 | Loss:1.3753\n",
      "[TRAIN] Epoch:060 | Loss:1.3639\n",
      "[TRAIN] Epoch:070 | Loss:1.3974\n",
      "[TRAIN] Epoch:080 | Loss:1.0698\n",
      "[TRAIN] Epoch:090 | Loss:1.0379\n",
      "[TRAIN] Epoch:100 | Loss:1.1293\n",
      "[TRAIN] Epoch:110 | Loss:1.0921\n",
      "[TRAIN] Epoch:120 | Loss:1.0248\n",
      "[TRAIN] Epoch:130 | Loss:1.3059\n",
      "[TRAIN] Epoch:140 | Loss:1.0535\n",
      "[TRAIN] Epoch:150 | Loss:0.9787\n",
      "[EVAL] trial:1 | MAE:1.4817\n",
      "[TRAIN] Epoch:010 | Loss:1.8821\n",
      "[TRAIN] Epoch:020 | Loss:1.6928\n",
      "[TRAIN] Epoch:030 | Loss:1.3718\n",
      "[TRAIN] Epoch:040 | Loss:1.3328\n",
      "[TRAIN] Epoch:050 | Loss:1.2586\n",
      "[TRAIN] Epoch:060 | Loss:1.1939\n",
      "[TRAIN] Epoch:070 | Loss:1.0933\n",
      "[TRAIN] Epoch:080 | Loss:1.0305\n",
      "[TRAIN] Epoch:090 | Loss:1.0487\n",
      "[TRAIN] Epoch:100 | Loss:1.1040\n",
      "[TRAIN] Epoch:110 | Loss:0.9935\n",
      "[TRAIN] Epoch:120 | Loss:1.0705\n",
      "[TRAIN] Epoch:130 | Loss:1.0205\n",
      "[TRAIN] Epoch:140 | Loss:1.2291\n",
      "[TRAIN] Epoch:150 | Loss:1.0224\n",
      "[EVAL] trial:2 | MAE:1.6503\n",
      "[FINAL EVAL] MAE: 1.5775+-0.0707\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [\n",
    "    \"train.py\",          \n",
    "    \"-dataset\", \"HOPV\",\n",
    "    \"-num_layer\", \"10\",\n",
    "    \"-hidden_dim\", \"512\",\n",
    "    \"-heads\", \"4\",\n",
    "    \"-num_epoch\", \"150\",\n",
    "    \"-batch_size\", \"16\",\n",
    "    \"-lr\", \"0.01\",\n",
    "    \"-num_trial\", \"3\"\n",
    "]\n",
    "\n",
    "args = parser.parse_args()                \n",
    "train(args, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd6c02-c657-4db1-b9ea-88e5c0853ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
